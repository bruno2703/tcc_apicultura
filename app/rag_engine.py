import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Carrega vari√°veis de ambiente
load_dotenv()

class ApiculturaRAG:
    def __init__(self):
        """
        Inicializa o motor RAG. Carrega o banco vetorial e o LLM na mem√≥ria.
        Isso roda apenas uma vez quando a API inicia.
        """
        print("üöÄ Inicializando Motor RAG do TCC...")
        
        # 1. Configura√ß√£o de Caminhos
        self.diretorio_atual = os.path.dirname(os.path.abspath(__file__))
        # Ajuste o caminho conforme sua estrutura: app/rag_engine.py -> data/chroma_db
        self.pasta_db = os.path.join(self.diretorio_atual, "..", "data", "chroma_db")
        
        # 2. Validar API Key
        if not os.getenv("GROQ_API_KEY"):
            raise ValueError("‚ùå ERRO CR√çTICO: GROQ_API_KEY n√£o encontrada no .env!")

        # 3. Carregar Embeddings (O mesmo usado na ingest√£o)
        print("‚è≥ Carregando Embeddings (MiniLM Multil√≠ngue)...")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'} # Use 'cpu' se n√£o tiver GPU no servidor
        )

        # 4. Conectar ao Banco de Vetores
        print(f"üìÇ Conectando ao ChromaDB em: {self.pasta_db}")
        self.vector_store = Chroma(
            persist_directory=self.pasta_db,
            embedding_function=self.embedding_model
        )
        
        # Recuperador otimizado (k=10 para maior precis√£o)
        self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 10})

        # 5. Configurar LLM (Llama 3 via Groq)
        print("üß† Configurando Llama 3 (Groq)...")
        self.llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0.2)

        # 6. Preparar a Chain (Prompt + L√≥gica)
        self.rag_chain = self._criar_chain()
        print("‚úÖ Motor RAG pronto para uso!")

    def _format_docs(self, docs):
        """Formata os documentos recuperados para o contexto do prompt."""
        formatted = []
        for i, doc in enumerate(docs, 1):
            fonte = doc.metadata.get('fonte', 'Desconhecida')
            formatted.append(f"[TRECHO {i} - Fonte: {fonte}]\n{doc.page_content}\n")
        return "\n".join(formatted)

    def _criar_chain(self):
        """Monta o pipeline de processamento LCEL."""
        template = """Voc√™ √© um Assistente T√©cnico Especialista em Apicultura no Semi√°rido Brasileiro (Caatinga).
        Sua fun√ß√£o √© ajudar produtores rurais com base APENAS nos manuais t√©cnicos fornecidos.

        REGRAS DE OURO:
        1. Leia TODOS os trechos do contexto abaixo.
        2. Combine informa√ß√µes de m√∫ltiplos trechos para criar uma resposta completa.
        3. Se a resposta n√£o estiver no contexto, diga: "Desculpe, n√£o encontrei essa informa√ß√£o nos manuais."
        4. Cite nomes de plantas ou t√©cnicas espec√≠ficas quando dispon√≠veis.
        5. Seja did√°tico e incentive boas pr√°ticas.

        Contexto dos Manuais:
        {context}

        Pergunta do Produtor:
        {question}

        Resposta T√©cnica:"""

        prompt = ChatPromptTemplate.from_template(template)

        # Chain LCEL
        chain = (
            {"context": self.retriever | self._format_docs, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        return chain

    def gerar_resposta(self, pergunta: str):
        """
        M√©todo p√∫blico que a API vai chamar.
        Recebe a pergunta e devolve a resposta + metadados.
        """
        try:
            # 1. RAG: Gera a resposta textual
            resposta_texto = self.rag_chain.invoke(pergunta)

            # 2. Fonte: Recupera os documentos usados (para citar fontes)
            docs_usados = self.retriever.invoke(pergunta)
            fontes_unicas = list(set(doc.metadata.get('fonte', 'Desconhecida') for doc in docs_usados))

            return {
                "resposta": resposta_texto,
                "fontes": fontes_unicas,
                "sucesso": True
            }
        except Exception as e:
            print(f"‚ùå Erro ao gerar resposta: {e}")
            return {
                "resposta": "Ocorreu um erro interno no servidor ao processar sua pergunta.",
                "fontes": [],
                "sucesso": False,
                "erro_detalhe": str(e)
            }

# Pequeno bloco de teste local (s√≥ roda se voc√™ executar este arquivo diretamente)
if __name__ == "__main__":
    motor = ApiculturaRAG()
    resultado = motor.gerar_resposta("Como alimentar abelhas na seca?")
    print("\n--- Teste Local ---")
    print(resultado['resposta'])
    print("\nFontes:", resultado['fontes'])